{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document Classification Test (HeavyWater Machine Learning Challenge)\n",
    "# LSTM models with document length side feature\n",
    "\n",
    "**Problem Statement**\n",
    "\n",
    "We process documents related to mortgages, aka everything that happens to originate a mortgage that you don't see as a borrower. Often times the only access to a document we have is a scan of a fax of a print out of the document. Our system is able to read and comprehend that document, turning a PDF into structured business content that our customers can act on.\n",
    "\n",
    "This dataset represents the output of the OCR stage of our data pipeline ...  Each word in the source is mapped to one unique value in the output. If the word appears in multiple documents then that value will appear multiple times. The word order for the dataset comes directly from our OCR layer, so it should be roughly in order.\n",
    "\n",
    "**Mission**\n",
    "\n",
    "Train a document classification model. Deploy your model to a public cloud platform (AWS/Google/Azure/Heroku) as a webservice, send us an email with the URL to you github repo, the URL of your publicly deployed service so we can submit test cases and a recorded screen cast demo of your solution's UI, its code and deployment steps. Also, we use AWS so we are partial to you using that ... just saying.\n",
    "\n",
    "**Lightweight way to test for tensorflow detection of GPUs (with diagnostics), using command line:**\n",
    "\n",
    "```python\n",
    "python3 -c \"from tensorflow.python.client import device_lib; print(device_lib.list_local_devices())\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "### Library import\n",
    "\n",
    "We import all the required Python libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T01:55:15.374498Z",
     "start_time": "2021-01-27T01:55:14.432558Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Jan 26 17:55:14 2021\n",
      "Python version:  (3, 6, 9, 'final', 0)\n",
      "Un-versioned imports:\n",
      "\n",
      "collections  gc  os  pathlib  pickle  random  sys\n",
      "\n",
      "colorcet: 1.0.0\tdateutil: 2.8.1\tgraphviz: 2.8.1\tmatplotlib: 3.3.3\tnumpy: 1.19.5\tpandas: 1.1.4\tre: 2.2.1\tscipy: 1.4.1\tseaborn: 0.11.1\tsklearn: 0.22.1\ttensorflow: 2.4.1\ttensorflow_addons: 0.12.0\t\n",
      "\n",
      "Δt:  0.0s.\n",
      "\n",
      "local devices:\n",
      "\n",
      " [name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 16223764030492155934\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 200736768\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "    link {\n",
      "      device_id: 1\n",
      "      type: \"StreamExecutor\"\n",
      "      strength: 1\n",
      "    }\n",
      "  }\n",
      "}\n",
      "incarnation: 11279135380970445939\n",
      "physical_device_desc: \"device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:08:00.0, compute capability: 6.1\"\n",
      ", name: \"/device:GPU:1\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 109182976\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "    link {\n",
      "      type: \"StreamExecutor\"\n",
      "      strength: 1\n",
      "    }\n",
      "  }\n",
      "}\n",
      "incarnation: 11562487159449151098\n",
      "physical_device_desc: \"device: 1, name: GeForce GTX 1080 Ti, pci bus id: 0000:42:00.0, compute capability: 6.1\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from time import asctime, gmtime, localtime, perf_counter\n",
    "print(asctime(localtime()))\n",
    "\n",
    "t0 = perf_counter()\n",
    "\n",
    "from collections import Counter, OrderedDict\n",
    "import gc\t\t# garbage collection module\n",
    "import os\n",
    "import pathlib\n",
    "import pickle\n",
    "from random import random\n",
    "import sys\n",
    "\n",
    "print(\"Python version: \", sys.version_info[:])\n",
    "print(\"Un-versioned imports:\\n\")\n",
    "prefixStr = ''\n",
    "print(prefixStr + 'collections', end=\"  \")\n",
    "print(prefixStr + 'gc', end=\"  \")\n",
    "print(prefixStr + 'os', end=\"  \")\n",
    "print(prefixStr + 'pathlib', end=\"  \")\n",
    "print(prefixStr + 'pickle', end=\"  \")\n",
    "print(prefixStr + 'random', end=\"  \")\n",
    "print(prefixStr + 'sys', end=\"\")\n",
    "\n",
    "import re\n",
    "\n",
    "from dateutil import __version__ as duVersion\n",
    "from dateutil.parser import parse\n",
    "import numpy as np\n",
    "\n",
    "mdVersion = None\n",
    "# from modin import __version__ as mdVersion\n",
    "# import modin.pandas as pd\n",
    "import pandas as pd\n",
    "ppVersion = None\n",
    "\n",
    "import graphviz\n",
    "\n",
    "scVersion = None\n",
    "from scipy import __version__ as scVersion\n",
    "import scipy.sparse as sp\n",
    "\n",
    "from sklearn import __version__ as skVersion\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "# from sklearn.metrics import f1_score, make_scorer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "tfVersion = None\n",
    "from tensorflow import __version__ as tfVersion\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras import Input\n",
    "from tensorflow.keras.layers import Dense, Embedding, Bidirectional, LSTM, concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.models import load_model as load\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.python.client import device_lib \n",
    "from tensorflow import device\n",
    "from tensorflow.keras.metrics import SparseCategoricalCrossentropy\n",
    "\n",
    "tfaVersion = None\n",
    "from tensorflow_addons import __version__ as tfaVersion\n",
    "from tensorflow_addons.metrics import F1Score\n",
    "\n",
    "# from joblib import __version__ as jlVersion\n",
    "# from joblib import dump, load\n",
    "\n",
    "# Visualizations\n",
    "\n",
    "mpVersion = None\n",
    "from matplotlib import __version__ as mpVersion\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import seaborn as sns\n",
    "import colorcet as cc\n",
    "\n",
    "print(\"\\n\")\n",
    "print(f\"colorcet: {cc.__version__}\", end=\"\\t\")\n",
    "print(f\"dateutil: {duVersion}\", end=\"\\t\")\n",
    "print(f\"graphviz: {duVersion}\", end=\"\\t\")\n",
    "# print(f\"joblib: {jlVersion}\", end=\"\\t\")\n",
    "print(f\"matplotlib: {mpVersion}\", end=\"\\t\")\n",
    "if 'modin' in sys.modules:\n",
    "    print(f\"modin: {mdVersion}\", end=\"\\t\")\n",
    "print(f\"numpy: {np.__version__}\", end=\"\\t\")\n",
    "if 'pandas' in sys.modules:\n",
    "    print(f\"pandas: {pd.__version__}\", end=\"\\t\")\n",
    "print(f\"re: {re.__version__}\", end=\"\\t\")\n",
    "print(f\"scipy: {scVersion}\", end=\"\\t\")\n",
    "print(f\"seaborn: {sns.__version__}\", end=\"\\t\")\n",
    "print(f\"sklearn: {skVersion}\", end=\"\\t\")\n",
    "print(f\"tensorflow: {tfVersion}\", end=\"\\t\")\n",
    "print(f\"tensorflow_addons: {tfaVersion}\", end=\"\\t\")\n",
    "\n",
    "Δt = perf_counter() - t0\n",
    "print(f\"\\n\\nΔt: {Δt: 4.1f}s.\")\n",
    "\n",
    "print(\"\\nlocal devices:\\n\\n\", device_lib.list_local_devices())\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# Options for pandas\n",
    "pd.options.display.max_columns = 30\n",
    "pd.options.display.max_rows = 50\n",
    "\n",
    "# Autoreload extension\n",
    "if 'autoreload' not in get_ipython().extension_manager.loaded:\n",
    "    %load_ext autoreload\n",
    "    \n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local library import\n",
    "\n",
    "We import all the required local libraries libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T01:13:09.444117Z",
     "start_time": "2021-01-27T01:13:08.978473Z"
    }
   },
   "outputs": [],
   "source": [
    "rootPath = pathlib.Path.cwd().parent\n",
    "libPath = rootPath / 'python'\n",
    "\n",
    "# Include local library paths\n",
    "sys.path.append(str(libPath)) # uncomment and fill to import local libraries\n",
    "\n",
    "# Import local libraries\n",
    "from utility import ModelTrain as mt\n",
    "from plotHelpers import plotHelpers as ph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"helper-tokenize\"></a>\n",
    "### Helper functions\n",
    "\n",
    "#### `tokenize()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(corpus, vocabSz):\n",
    "    \"\"\"\n",
    "    Generates the vocabulary and the list of list of integers for the input corpus\n",
    "\n",
    "    Help from: https://www.tensorflow.org/tutorials/text/nmt_with_attention\n",
    "\n",
    "    INPUTS:\n",
    "        corpus: list, type(str), containing (short) document strings\n",
    "        vocabSz: (int) Maximum number of words to consider in the vocabulary\n",
    "\n",
    "    RETURNS: List of list of indices for each string in the corpus + Keras sentence tokenizer object\n",
    "\n",
    "    Usage:\n",
    "        listOfListsOfIndices, sentenceTokenizer = tokenize(mySentences, maxVocabCt)\n",
    "    \"\"\"\n",
    "\n",
    "    # Define the sentence tokenizer\n",
    "    tokenizer = Tokenizer(num_words=vocabSz,\n",
    "    #                               filters='!#%()*+,./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n",
    "                                  filters='%',\n",
    "                                  lower=False,\n",
    "                                  split=' ', char_level=False, oov_token=\"<unkwn>\")\n",
    "\n",
    "    # Keep the double quote, dash, and single quote + & (different from word2vec training: didn't keep `&`)\n",
    "    # oov_token: added to word_index & used to replace out-of-vocab words during text_to_sequence calls\n",
    "    # num_words = maximum number of words to keep, dropping least frequent\n",
    "\n",
    "    # Fit the tokenizer on the input corpus\n",
    "    tokenizer.fit_on_texts(corpus)\n",
    "\n",
    "    # Transform each text in corpus to a sequence of integers\n",
    "    listOfIndexLists = tokenizer.texts_to_sequences(corpus)\n",
    "\n",
    "    return listOfIndexLists, tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data\n",
    "\n",
    "#### Define paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataPath = rootPath / 'data'\n",
    "modelPath = rootPath / 'model'\n",
    "plotPath = rootPath / 'figures'\n",
    "checkpointPath = rootPath / 'checkpoints'\n",
    "tensorBoardPath = rootPath / 'tensorBoardLogs'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sourceData = dataPath / 'shuffled-full-set-hashed.csv.zip'\n",
    "df0 = pd.read_csv(sourceData, header=None, names=['category', 'docText'])\n",
    "df0.head()\n",
    "df0.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Munge/inspect data\n",
    "\n",
    "**There are 45 null documents**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df0.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**There are 14 document categories**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = df0.category.unique()\n",
    "len(categories)\n",
    "print(categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract tokens (in order to get document lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df0['tokens'] = df0.docText.apply(lambda p: [] if isinstance(p, float) else p.split())\n",
    "df0.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get token counts (side feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df0['docLength'] = df0.tokens.apply(lambda t: len(t))\n",
    "df0.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-process data\n",
    "\n",
    "### Test-train split\n",
    "\n",
    "* remove documents of length < <font color=\"darkred\">**6**</font>:\n",
    "  * these are unlikely to be informative, and probably are result of scan error\n",
    "  * probably should have these labeled as an error, for human review, rather than risk downstream adoption\n",
    "* class imbalance spanning almost 2 orders of magnitude ⟶ *stratified sampling*\n",
    "* smallest classes 229 instances, so need half to test with ~10% uncertainty\n",
    "* after model selection, can train on entire data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df0.copy()[df0.docLength > 5]\n",
    "df0.shape, df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create list of lists of word indices, and TensorFlow sentence tokenizer object\n",
    "\n",
    "Use strings from `dfTrain` to create vocabulary indices.\n",
    "\n",
    "See [helper function `tokenize()`](#helper-tokenize)\n",
    "\n",
    "* Each token is 12 characters long, so minimum string length is 6 &times; 12 + 5 (spaces) = 77\n",
    "\n",
    "<a id=\"maxvocabct\"></a>\n",
    "Must specify a limit to the number of unique tokens for the tokenizer.\n",
    "(Changing this will require re-instantiating it.)\n",
    "\n",
    "* `maxVocabCt`\t\t\tvocabulary size to be returned by tokenizer, dropping least frequent\n",
    "\n",
    "Other parameters are defined below in [LSTM 0, baseline model parameters](#lstm0-parameters), and similarly for subsequent models.\n",
    "\n",
    "Tokenizing takes ~10 s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxVocabCt = 200_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.docText.str.len().min()\n",
    "ListOfDocsTr = list(dfTr.docText)\n",
    "listOfListsOfWordIndicesTr, tokenizer = tokenize(ListOfDocsTr, maxVocabCt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute weights for each class\n",
    "\n",
    "#### `dfTr` category breakdowns\n",
    "\n",
    "* categoriesBySupport are category names ordered by support in `dfTr`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categoryCts = dfTr[['category', 'docLength']].groupby(by='category').count()\\\n",
    "    .rename(columns={'docLength': 'count'})\n",
    "categoryCts\n",
    "\n",
    "categoryCts.sort_values(by='count', ascending=False)\n",
    "categoriesBySupport = list(categoryCts.sort_values(by='count', ascending=False).index)\n",
    "categoriesBySupport"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract training and test labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categoryInds = {c: i for c, i in zip(categoriesBySupport, range(len(categories)))}\n",
    "\n",
    "yTr = dfTr.category.apply(lambda c: categoryInds[c])\n",
    "yTe = dfTe.category.apply(lambda c: categoryInds[c])\n",
    "yTr.head()\n",
    "yTr.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Determine class weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = class_weight.compute_class_weight('balanced',\n",
    "                                            range(len(categories)),\n",
    "                                            yTr)\n",
    "print(weights)\n",
    "classWeights = {i: weights[i] for i in range(len(categories))}\n",
    "print(\"classWeights:\\n\", classWeights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM model(s)\n",
    "\n",
    "* This/these differ from simple models, as they include a side feature `docLength`\n",
    "* Since a substantial fraction of documents have lengths `docLength > maxDocWords`, this feature should be informative\n",
    "\n",
    "<a id=\"define-model1\"></a>\n",
    "### Define model1\n",
    "\n",
    "* embedding layer\n",
    "* bidirectional LSTM\n",
    "* unidirectional LSTM *(optional)*\n",
    "* dense layer (relu)\n",
    "* dense layer (relu)\n",
    "* classifier dense layer (softmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model1(sequence_length, vocabSz, auxFeatureCount, LSTMinternalLayerSz,\n",
    "           embedLayerDim, densLayerDim=64, softMaxCt=16, dropoutFrac=0.15,\n",
    "           LSTMdropoutFrac=0.40, include2ndLSTMlayer=False):\n",
    "\n",
    "    \"\"\"\n",
    "    INPUTS:\n",
    "    sequence_length\t\t\tint, number of LSTM units\n",
    "    vocabSz\t\t\t\t\tint, size of vocabulary\n",
    "    auxFeatureCount\t\t\tint, count of auxiliary (side) features\n",
    "    LSTMinternalLayerSz\t\tint, size of layers within LSTM units\n",
    "    embedLayerDim\t\t\tint, dimension of embedding layer\n",
    "    densLayerDim\t\t\tint, dimension of dense layers, default: 64\n",
    "    softMaxCt\t\t\t\tint, dimension of softmax output, default: 16\n",
    "    dropoutFrac\t\t\t\tint, dropout rate, default: 0.15\n",
    "    LSTMdropoutFrac\t\t\tint, dropout rate for LSTMs, default: 0.40\n",
    "    include2ndLSTMlayer\t\tbool, include unidirectional LSTM after\n",
    "                            bidirectional LSTM, default: False\n",
    "    \"\"\"\n",
    "\n",
    "    # Headline input: meant to receive sequences of *sequence_length*\n",
    "    # integers, between 1 and *vocabSz*.\n",
    "\n",
    "    main_input = Input(shape=(sequence_length,), dtype='int32', name='MainInput')\n",
    "    auxiliary_input = Input(shape=(auxFeatureCount,), name='NumericalInput')\n",
    "\n",
    "    # This embedding layer will encode the input sequence\n",
    "    # into a sequence of dense 64-dimensional vectors.\n",
    "    x = Embedding(output_dim=embedLayerDim, input_dim=vocabSz,\n",
    "                  input_length=sequence_length, trainable=True, name=\"EmbedLayer\")(main_input)\n",
    "\n",
    "    # A LSTM will transform the vector sequence into a single vector,\n",
    "    # containing information about the entire sequence\n",
    "    lstmOut0 = Bidirectional(LSTM(LSTMinternalLayerSz,\n",
    "                                    dropout=dropoutFrac,\n",
    "                                    recurrent_dropout=LSTMdropoutFrac,\n",
    "                                    return_sequences=False), name='BidirectionalLSTM')(x)\n",
    "                                    # return_sequences=True), name='BidirectionalLSTM')(x)\n",
    "    if not include2ndLSTMlayer:\n",
    "        x = concatenate([lstmOut0, auxiliary_input], name='ConcatenatedFeatures')\n",
    "    else:\n",
    "        # Add a second, unidirectional LSTM, if desired\n",
    "        lstmOut1 = LSTM(LSTMinternalLayerSz,\n",
    "                        dropout=dropoutFrac,\n",
    "                        recurrent_dropout=LSTMdropoutFrac, name='UnidirectionalLSTM')(lstmOut0)\n",
    "        x = concatenate([lstmOut1, auxiliary_input], name='ConcatenatedFeatures')\n",
    "\n",
    "    # We stack a deep densely-connected network on top\n",
    "    x = Dense(densLayerDim, activation='relu', name='DenseLayer0')(x)\n",
    "    x = Dense(densLayerDim, activation='relu', name='DenseLayer1')(x)\n",
    "\n",
    "    # And finally we add the main logistic regression layer\n",
    "    main_output = Dense(56, activation='softmax', name='mainOutput')(x)\n",
    "    model = Model(inputs=[main_input, auxiliary_input], outputs=main_output)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"tokenize-512-tokens\"></a>\n",
    "### LSTM 6 tokenize\n",
    "\n",
    "* truncate docs to `maxDocWords = 512` tokens\n",
    "* pre-pad shorter docs with 0s\n",
    "\n",
    "##### Tensor of word indices for train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "padValue = 0\n",
    "maxDocWords = 512\n",
    "\n",
    "XdocsTr = pad_sequences(listOfListsOfWordIndicesTr,\n",
    "                        maxlen=maxDocWords,\n",
    "                        dtype='int32', padding='pre',\n",
    "                        truncating='post', value=padValue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ListOfDocsTr[0]\n",
    "print(listOfListsOfWordIndicesTr[0])\n",
    "XdocsTr[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tensor of word indices for test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ListOfDocsTe = list(dfTe.docText)\n",
    "listOfListsOfWordIndicesTe = tokenizer.texts_to_sequences(ListOfDocsTe)\n",
    "XdocsTe = pad_sequences(listOfListsOfWordIndicesTe,\n",
    "                        maxlen=maxDocWords,\n",
    "                        dtype='int32', padding='pre',\n",
    "                        truncating='post', value=padValue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ListOfDocsTe[0]\n",
    "print(listOfListsOfWordIndicesTe[0])\n",
    "XdocsTe[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Auxiliary (side) data need to be shaped\n",
    "\n",
    "* creates a row vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XauxTr = dfTr.docLength.values.reshape(dfTr.shape[0], 1)\n",
    "XauxTr.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"lstm6-parameters\"></a>\n",
    "### LSTM 6\n",
    "\n",
    "#### baseline model parameters\n",
    "\n",
    "* Bidirectional(LSTM) only\n",
    "* `LSTMlayerUnits = 128`\n",
    "* `maxDocWords = 512`\n",
    "\n",
    "Refer to the [LSTM 6 tokenize](#tokenize-512-tokens) section for the size of `maxVocabCt`.\n",
    "\n",
    "|parameter|&nbsp;&nbsp;|description|\n",
    "|:--------|------------|:----------|\n",
    "|`testFrac`||fraction of data set withheld|\n",
    "|`LSTMlayerUnits`||# units within each activation unit in LSTMs|\n",
    "|`embeddingDim'||size of dimension for generated embeddings|\n",
    "|`auxFeaturesCt`||# of features in auxiliary data|\n",
    "|`classCt`||# classes (softmax output dim)|\n",
    "|`auxFeatureCount`||# of side features|\n",
    "|`dropoutFrac`||dropout fraction|\n",
    "|`LSTMdropoutFrac`||dropout fraction within LSTMs|\n",
    "|`batchSz`||size of batches|\n",
    "|`epochCt`||number of epochs to run|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testFrac = 0.5\n",
    "LSTMlayerUnits = 128\t\t# 🢢🢢🢢\n",
    "embeddingDim = 64\n",
    "classCt = len(categoriesBySupport)\n",
    "auxFeatureCount = 1\n",
    "dropoutFrac = 0.15\n",
    "LSTMdropoutFrac = 0.5\n",
    "# LSTMdropoutFrac = 0\t\t\t# Must be 0 for use of cudnn\n",
    "batchSz = 64\n",
    "epochCt = 30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T01:49:04.773746Z",
     "start_time": "2021-01-27T01:49:04.771071Z"
    }
   },
   "outputs": [],
   "source": [
    "# del LSTMX\t\t# (placeholder, in case this section copied for subsequent models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LSTM 6 callbacks\n",
    "\n",
    "* checkpoints\n",
    "* TensorBoard\n",
    "* (no early stopping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelInstanceDir = (f\"vocabCt{maxVocabCt:06d}maxCommentLen{maxDocWords:03d}\"\n",
    "                    + f\"classCt{classCt:02d}\"\n",
    "                    + f\"embedDim{embeddingDim:03d}\"\n",
    "                    + f\"LSTMlayerSz{LSTMlayerUnits:03d}batchSz{batchSz:03d}\"\n",
    "                    + f\"dropoutFrac{dropoutFrac:4.2f}\"\n",
    "                    + f\"LSTMdropoutFrac{dropoutFrac:4.2f}\")\n",
    "print(modelInstanceDir, \"\\n\")\n",
    "\n",
    "checkpointPrefix = os.path.join(checkpointPath, modelInstanceDir,\n",
    "                                \"ckpt{epoch:03d}\")\n",
    "print(checkpointPrefix, \"\\n\")\n",
    "\n",
    "checkpointCallback=ModelCheckpoint(filepath=checkpointPrefix,\n",
    "                                   save_weights_only=True)\n",
    "os.makedirs(tensorBoardPath, exist_ok=True)                       \n",
    "\n",
    "logsDir = os.path.join(tensorBoardPath, modelInstanceDir)\n",
    "print(logsDir, \"\\n\")\n",
    "\n",
    "os.makedirs(logsDir, exist_ok=True)\n",
    "tensorboardCallback = TensorBoard(log_dir=logsDir, histogram_freq=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load or instantiate LSTM 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T01:49:04.773746Z",
     "start_time": "2021-01-27T01:49:04.771071Z"
    }
   },
   "outputs": [],
   "source": [
    "LSTM6name = 'LSTM6'\n",
    "\n",
    "if (modelPath / LSTM6name).is_file():\n",
    "    print(f\"Loading {LSTM6name} model from disk.\")\n",
    "    LSTM6 = load(modelPath / LSTM6name)\n",
    "else:\n",
    "    np.random.seed(0)  # Set a random seed for reproducibility\n",
    "\n",
    "    print(\"Instantiate LSTM 6, using model0 ...\")\n",
    "    with device('/device:GPU:1'):\n",
    "        LSTM6 = model1(maxDocWords, maxVocabCt, auxFeatureCount, classCt, LSTMlayerUnits,\n",
    "                       embeddingDim, softMaxCt=classCt)\n",
    "    LSTM6.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"model1-graph\"></a>\n",
    "#### Model 1 graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(LSTM6, to_file=os.path.join(plotPath, 'model0graph.png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compile LSTM 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not (modelPath / LSTM3name).is_file():\n",
    "    with device('/device:GPU:1'):\n",
    "      LSTM3.compile(optimizer='rmsprop',\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  # metrics = ['accuracy', Recall(), Precision(),\n",
    "                  #            F1Score(num_classes=classCt), 'categorical_crossentropy'])\n",
    "                  metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train LSTM 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T01:49:04.773746Z",
     "start_time": "2021-01-27T01:49:04.771071Z"
    }
   },
   "outputs": [],
   "source": [
    "if not (modelPath / LSTM6name).is_file():\n",
    "    print(epochCt, batchSz)\n",
    "    print(classWeights)\n",
    "    with device('/device:GPU:1'):\n",
    "        history6 = LSTM6.fit(x=[XdocsTr, XauxTr],\n",
    "                             y= yTr.values,\n",
    "                             epochs=epochCt, batch_size=batchSz,\n",
    "                             shuffle=True,\n",
    "                             class_weight=classWeights,\n",
    "                             validation_split=0.2,\n",
    "                             callbacks=[checkpointCallback, tensorboardCallback],\n",
    "                             verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save LSTM 6, if new model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not (modelPath / LSTM6name).is_file():\n",
    "    print(f\"Saving {LSTM6name} to disk.\")\n",
    "    LSTM6.save(modelPath / LSTM6name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LSTM 6 inference on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmaxOut = LSTM6.predict(x=XdocsTe)\n",
    "yPred = np.argmax(softmaxOut, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusionMat = confusion_matrix(yTe, yPred)\n",
    "print(confusionMat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.where(np.sum(confusionMat, axis=0) == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = np.trace(confusionMat)/np.sum(confusionMat)\n",
    "recall = np.diag(confusionMat)/np.sum(confusionMat, axis=1)\n",
    "precision = np.diag(confusionMat)/np.sum(confusionMat, axis=0)\n",
    "print(f\"accuracy: {accuracy:0.3f}, \"\n",
    "      f\"<precision>: {np.mean(precision):0.3f}, \"\n",
    "      f\"<recall>: {np.mean(recall):0.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Classification report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classificationReport = classification_report(yTe.values, yPred,\n",
    "                                             target_names=[str(c)for c in categoriesBySupport])\n",
    "print(classificationReport)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Sorted classification report\n",
    "\n",
    "* order by support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ph.sortClassificationReport(classificationReport))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Plot confusion matrix\n",
    "\n",
    "* As this is a straight confusion matrix, diagonal elements mostly reflect class size in test set\n",
    "* *This is hard to interpret by visual inspection alone*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelFontSz = 16\n",
    "tickFontSz = 13\n",
    "titleFontSz = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(12, 12))\n",
    "ph.plotConfusionMatrix(confusionMat, saveAs=None, xlabels=categories,\n",
    "                       ylabels=categories, titleText = 'LSTM 6',\n",
    "                       ax = ax,  xlabelFontSz=labelFontSz,\n",
    "                       xtickRotate=0.65, ytickRotate=0.0,\n",
    "                       ylabelFontSz=labelFontSz, xtickFontSz=tickFontSz,\n",
    "                       ytickFontSz=tickFontSz, titleFontSz=titleFontSz)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Plot recall confusion matrix\n",
    "\n",
    "* normalized by *row*\n",
    "* diagonal elements now represent the *recall* for each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(12, 12))\n",
    "ph.plotConfusionMatrix(confusionMat, saveAs=None, xlabels=categories,\n",
    "                       ylabels=categories, titleText = 'LSTM 6',\n",
    "                       ax = ax, xlabelFontSz=labelFontSz,\n",
    "                       xtickRotate=0.65, ytickRotate=0.0, type='recall',\n",
    "                       ylabelFontSz=labelFontSz, xtickFontSz=tickFontSz,\n",
    "                       ytickFontSz=tickFontSz, titleFontSz=titleFontSz)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Plot precision confusion matrix\n",
    "\n",
    "* normalized by *column*\n",
    "* diagonal elements now represent the *precision* for each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(12, 12))\n",
    "ph.plotConfusionMatrix(confusionMat, saveAs=None, xlabels=categories,\n",
    "                       ylabels=categories, titleText = 'LSTM 6',\n",
    "                       ax = ax,  xlabelFontSz=labelFontSz,\n",
    "                       xtickRotate=0.65, ytickRotate=0.0, type='precision',\n",
    "                       ylabelFontSz=labelFontSz, xtickFontSz=tickFontSz,\n",
    "                       ytickFontSz=tickFontSz, titleFontSz=titleFontSz)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
